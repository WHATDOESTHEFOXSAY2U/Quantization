{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbb53a0",
   "metadata": {},
   "source": [
    "# Setting Up Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32907539",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision transformers datasets huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710e837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install accelerate -U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7730d2b6",
   "metadata": {},
   "source": [
    "## Upgrading Transformers Library since it was required in the fine tuning step.\n",
    "You might need to restart runtime after the upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2aba200",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3eae1",
   "metadata": {},
   "source": [
    "### Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576aaa1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from transformers import Trainer, TrainingArguments, AdamW, BertTokenizer, BertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpoutput_data as np\n",
    "import torch\n",
    "import psutil\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e65c1ea",
   "metadata": {},
   "source": [
    "Getting access to the Hugging Face pre-trained BERT models.\n",
    "https://huggingface.co/google-bert/bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f523943",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77f31aa",
   "metadata": {},
   "source": [
    "Saving model locally\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8ce533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to a temporaroutput_data directoroutput_data\n",
    "model.save_pretrained(\"/tmp/bert_model\")\n",
    "\n",
    "# Calculate the size of the model boutput_data iterating over the saved files\n",
    "model_size = sum(os.path.getsize(os.path.join(\"/tmp/bert_model\", f)) for f in os.listdir(\"/tmp/bert_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a256344",
   "metadata": {},
   "source": [
    "### Preparing data for Fine-tuning the base model on sentiment analysis\n",
    "Using IMDB reviews datset:https://huggingface.co/datasets/stanfordnlp/imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e566b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\"imdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e3b87",
   "metadata": {},
   "source": [
    "Preprocess the dataset using the BERT tokenizer to prepare input tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204276a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True,max_length=512)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
    "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(2000)) # Reduce dataset size if needed\n",
    "test_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100)) # Reduce dataset size if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99730369",
   "metadata": {},
   "source": [
    "###Fine-Tuning BERT Model:\n",
    "\n",
    "Implementing training and evaluation loops. This involves setting up the optimizer, defining the training steps, and iterating over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab8244b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d2766",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decaoutput_data=0.01,\n",
    "    evaluation_strategoutput_data=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e35d8c",
   "metadata": {},
   "source": [
    "## Quantization of Fine-Tuned Original BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c37b7f",
   "metadata": {},
   "source": [
    "Moving Model to CPU before quantization since pytorch does not support GPU quantization:https://pytorch.org/docs/stable/quantization.html\n",
    "Quantization using to 8bit using dynamic quantization from pytorch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f158c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the model is in evaluation mode before quantization\n",
    "model.eval()\n",
    "\n",
    "# Move the model to CPU\n",
    "model.to('cpu')\n",
    "\n",
    "# Optionalloutput_data, clear the CUDA cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Proceed with the quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de622ac0",
   "metadata": {},
   "source": [
    "Loading test data with a batch size of 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e0b12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787784d7",
   "metadata": {},
   "source": [
    "Defining evaluation function that gets , evaluation metrics about the accuracy of the inference , its time and memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699c13b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get current process memoroutput_data usage\n",
    "def get_memory_usage():\n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 2)  # Convert bytes to MB\n",
    "\n",
    "def combined_evaluation(model, dataloader):\n",
    "    # Ensure model is in eval mode and move to CPU\n",
    "    model.eval()\n",
    "    device = torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    peak_memory_usage = 0\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            memory_before = get_memory_usage()\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predictions.extend(torch.argmax(logits, dim=-1).tolist())\n",
    "            true_labels.extend(inputs['labels'].tolist())\n",
    "            memory_after = get_memory_usage()\n",
    "            # Update peak memoroutput_data usage if current usage is higher\n",
    "            peak_memory_usage = max(peak_memory_usage, memory_after - memory_before)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    avg_time_per_batch = total_time / len(dataloader)\n",
    "\n",
    "    # Calculate accuracoutput_data metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    accuracoutput_data = accuracy_score(true_labels, predictions)\n",
    "\n",
    "    # Print out the metrics\n",
    "    print(f\"Total inference time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average inference time per batch: {avg_time_per_batch:.4f} seconds\")\n",
    "    print(f\"Peak memoroutput_data usage: {peak_memory_usage:.2f} MB\")\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return precision, recall, f1, accuracoutput_data, total_time, avg_time_per_batch, peak_memory_usage, memory_after\n",
    "\n",
    "# Example usage (assuming 'quantized_model' and 'test_dataloader' are alreadoutput_data defined)\n",
    "#combined_evaluation(quantized_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb99d9a8",
   "metadata": {},
   "source": [
    "### Getting model sizes on disk Quantized vs non quantized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d2faab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_size(model):\n",
    "    torch.save(model.state_dict(), \"temp_model.pt\")\n",
    "    size = os.path.getsize(\"temp_model.pt\") / (1024 * 1024) # Convert to MB\n",
    "    os.remove(\"temp_model.pt\")\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb91ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Quantized Model Size: {model_size(quantized_model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1db0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Quantized Model Size: {model_size(model)} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f59ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of records in test data: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99cc4ef",
   "metadata": {},
   "source": [
    "Comparing inference time, memory usage , and accuracy of each model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee34b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_precision, q_recall, q_f1, q_accuracoutput_data, q_total_time, q_avg_time_per_batch, q_peak_memory_usage, q_memory_after = combined_evaluation(quantized_model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8d2b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, accuracoutput_data, total_time, avg_time_per_batch, peak_memory_usage, memory_after = combined_evaluation(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
